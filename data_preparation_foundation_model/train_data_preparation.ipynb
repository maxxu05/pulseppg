{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c12029",
   "metadata": {},
   "source": [
    "##### Processing PPG Data from Parquet Files\n",
    "\n",
    "<ul style=\"font-size: 0.8em;\">\n",
    "This following code snippet processes Photoplethysmography (PPG) data stored in Parquet files. It extracts relevant data columns from each file, consolidates the information, and saves the structured data as pickle files. The key steps include:\n",
    "\n",
    "  <li><strong>Reading Parquet Files:</strong> The script reads specific columns from each Parquet file to optimize performance.</li>\n",
    "  <li><strong>Data Consolidation:</strong> Data from multiple files is concatenated into a single DataFrame, ensuring consistency and adding user identifiers.</li>\n",
    "  <li><strong>Data Cleaning:</strong> The DataFrame is cleaned by removing duplicates and sorting by local time.</li>\n",
    "  <li><strong>Grouping and Saving:</strong> The cleaned data is grouped by day and start hour, then each group is saved as a separate pickle file.</li>\n",
    "\n",
    "This process is executed for each user directory in parallel, leveraging multiple cores for efficient processing.  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45927bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  \n",
    "from tqdm.auto import tqdm  # For automatic selection of appropriate tqdm implementation\n",
    "\n",
    "\n",
    "def extract_ppg_df(udir: str, input_dir: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Extracts PPG (Photoplethysmography) data from parquet files within a user directory,\n",
    "    processes it, and saves the resulting DataFrames as pickle files grouped by day and start hour.\n",
    "\n",
    "    Parameters:\n",
    "    - udir (str): User directory name.\n",
    "    - input_dir (str): Path to the input directory containing user directories.\n",
    "    - output_dir (str): Path to the output directory where processed files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    udir_path = os.path.join(input_dir, udir)\n",
    "    user = udir.split('=')[1]  # Extract user identifier from directory name\n",
    "\n",
    "    all_data = []\n",
    "    # List all parquet files within the user directory\n",
    "    files = [file for file in os.listdir(udir_path) if file.endswith('parquet')]\n",
    "    \n",
    "    # Process each parquet file\n",
    "    for file in tqdm(files, desc=f\"Processing parquet files for {udir}\", leave=True):\n",
    "        filepath = os.path.join(udir_path, file)\n",
    "        # Read specific columns from parquet file for efficiency\n",
    "        df = pd.read_parquet(filepath, columns=['localtime', 'ppg1', 'day', 'start_hour', 'end_hour'])\n",
    "        all_data.append(df)\n",
    "        \n",
    "    # Concatenate all data into a single DataFrame\n",
    "    ppg_df = pd.concat(all_data, ignore_index=True)\n",
    "    ppg_df['user'] = user  # Add user identifier column\n",
    "\n",
    "    # Clean and organize data by removing duplicates and sorting\n",
    "    ppg_df = (ppg_df\n",
    "              .drop_duplicates()\n",
    "              .sort_values(by='localtime')\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    # Group data by day and start hour, then save each group as a pickle file\n",
    "    grp_dfs = ppg_df.groupby(['day', 'start_hour'])\n",
    "    for (day, st_hr), df in grp_dfs:\n",
    "        filename = f\"{user}_{day}_{st_hr}.pickle\"\n",
    "        with open(os.path.join(output_dir, filename), 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = os.path.join(os.getcwd(), 'SSL_ppg_100_days')\n",
    "output_dir = os.path.join(os.getcwd(), 'dfs_ppg_100_days')\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "# Get list of user directories\n",
    "user_dirs = [udir for udir in os.listdir(input_dir) if udir.startswith('user')]\n",
    "\n",
    "# Process each user directory in parallel\n",
    "list(\n",
    "    tqdm(\n",
    "        Parallel(return_as=\"generator\", n_jobs=10)(\n",
    "            delayed(extract_ppg_df)(udir, input_dir, output_dir) \n",
    "            for udir in tqdm(user_dirs, desc=\"Processing for uid: {udir}\")\n",
    "        ),\n",
    "        total=len(user_dirs),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cae125",
   "metadata": {},
   "source": [
    "##### Create a dataframe of file names of stored pickle files. For consistency and seamless transition to foundation models research with multi-modality data (PPG + ACC), match PPG files with corresponding ACC files for the same day and hour of a user. For efficient matching, parquet files were converted to pickle files rather than directly to numpy files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb27b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>00222a15-7274-34b7-990a-81ac7d742220</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>2022-09-20 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18461</th>\n",
       "      <td>00222a15-7274-34b7-990a-81ac7d742220</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>2022-09-20 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21814</th>\n",
       "      <td>00222a15-7274-34b7-990a-81ac7d742220</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>2022-09-20 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31237</th>\n",
       "      <td>00222a15-7274-34b7-990a-81ac7d742220</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>2022-09-20 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27271</th>\n",
       "      <td>00222a15-7274-34b7-990a-81ac7d742220</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>2022-09-20 15:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user        day                hour\n",
       "24782  00222a15-7274-34b7-990a-81ac7d742220 2022-09-20 2022-09-20 11:00:00\n",
       "18461  00222a15-7274-34b7-990a-81ac7d742220 2022-09-20 2022-09-20 12:00:00\n",
       "21814  00222a15-7274-34b7-990a-81ac7d742220 2022-09-20 2022-09-20 13:00:00\n",
       "31237  00222a15-7274-34b7-990a-81ac7d742220 2022-09-20 2022-09-20 14:00:00\n",
       "27271  00222a15-7274-34b7-990a-81ac7d742220 2022-09-20 2022-09-20 15:00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = os.path.join(os.getcwd(), 'dfs_ppg_100_days')\n",
    "PPG_available = []\n",
    "for filename in os.listdir(output_dir):\n",
    "    PPG_available.append(filename.split('.')[0].split('_'))\n",
    "\n",
    "PPG_available_df = (pd.DataFrame(PPG_available, columns=['user', 'day', 'hour'])\n",
    "                    .astype({'user': str, \n",
    "                             'day': 'datetime64[ns]',  # covert to datetime from object type\n",
    "                             'hour': 'datetime64[ns]'}) # covert to datetime from object type\n",
    "                    .sort_values(by=['user', 'day', 'hour']))\n",
    "\n",
    "PPG_available_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72441047",
   "metadata": {},
   "source": [
    "##### Match available PPG and ACC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca66c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 users with 72609 available ACC files\n",
      "120 users with 72649 available PPG files\n"
     ]
    }
   ],
   "source": [
    "with open(\"ACC_available_df.pickle\", 'rb') as f:\n",
    "    ACC_available_df = pickle.load(f)\n",
    "    \n",
    "print(f\"{ACC_available_df['user'].nunique()} users with {len(ACC_available_df)} available ACC files\")\n",
    "print(f\"{PPG_available_df['user'].nunique()} users with {len(PPG_available_df)} available PPG files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f627da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 users with 72590 matching PPG, ACC files\n"
     ]
    }
   ],
   "source": [
    "PPG_ACC_available = pd.merge(PPG_available_df, ACC_available_df, \n",
    "                             on=['user', 'day', 'hour'], \n",
    "                             how='inner')\n",
    "print(f\"{PPG_ACC_available['user'].nunique()} users with {len(PPG_ACC_available)} matching PPG, ACC files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a125237",
   "metadata": {},
   "source": [
    "##### Combining PPG and ACC Data for Processing\n",
    "\n",
    "<ul style=\"font-size: 0.8em;\">\n",
    "This code snippet efficiently combines Photoplethysmography (PPG) and Accelerometer (ACC) data for specified time windows and saves the results as numpy arrays. The process involves:\n",
    "\n",
    "  <li><strong>Reading and Sorting Data:</strong> PPG and ACC data are read from pickled DataFrames and sorted by local time.</li>\n",
    "  <li><strong>Windowed Processing:</strong> Data is processed in time windows, ensuring sufficient data points and performing necessary downsampling or interpolation.</li>\n",
    "  <li><strong>Data Combination:</strong> The PPG and ACC data are combined into a single array for each window.</li>\n",
    "  <li><strong>Parallel Execution:</strong> The operation is executed in parallel using joblib to enhance performance, especially when processing large datasets.</li>\n",
    "\n",
    "This approach allows for the efficient handling and analysis of large-scale PPG and ACC datasets for dynamic window sizes. The same code snippet can be used for 1-min, 2-min training data preparation with corresponding directory names. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def combined_ppg_acc(row: pd.Series, PPG_df_directory: str, ACC_df_directory: str, output_np_dir: str, window_size: int) -> None:\n",
    "    \"\"\"\n",
    "    Combines PPG and ACC data for a given time window and saves the result as a numpy file.\n",
    "\n",
    "    Parameters:\n",
    "    - row (pd.Series): A row from a DataFrame containing 'user', 'day', and 'hour' information.\n",
    "    - PPG_df_directory (str): Directory path where PPG DataFrames are stored.\n",
    "    - ACC_df_directory (str): Directory path where ACC DataFrames are stored.\n",
    "    - output_np_dir (str): Directory path to save the combined numpy arrays.\n",
    "    - window_size (int): Size of the time window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    user = row['user']        \n",
    "    day = row['day'].date()\n",
    "    day_hr = row['hour']\n",
    "    \n",
    "    file = f\"{user}_{day}_{day_hr}.pickle\"\n",
    "    \n",
    "    # Load the PPG and ACC data\n",
    "    with open(os.path.join(PPG_df_directory, file), \"rb\") as f:\n",
    "        PPG_df = pickle.load(f)\n",
    "    with open(os.path.join(ACC_df_directory, file), \"rb\") as f:\n",
    "        ACC_df = pickle.load(f)\n",
    "\n",
    "    # Sort data by local time\n",
    "    PPG_df = PPG_df.sort_values(by=['localtime'])\n",
    "    ACC_df = ACC_df.sort_values(by=['localtime'])\n",
    "\n",
    "    # Initialize start and end times for the window\n",
    "    st_ppg = PPG_df['localtime'].iloc[0]\n",
    "    et_ppg = st_ppg + timedelta(seconds=window_size)\n",
    "    st_acc = ACC_df['localtime'].iloc[0]\n",
    "    et_acc = st_acc + timedelta(seconds=window_size)\n",
    "\n",
    "    # Process each time window\n",
    "    while et_ppg <= PPG_df['localtime'].iloc[-1] and et_acc <= ACC_df['localtime'].iloc[-1]:\n",
    "        win_min = st_ppg.minute\n",
    "        \n",
    "        # If the window_size is less than 60 seconds, the filename pattern f\"{user}/{day}/{day_hr}/{day_hr}:{win_min}.npy\" \n",
    "        # could lead to file overwrites, as multiple 10-second windows within the same minute would have identical filenames. \n",
    "        # Therefore, including seconds in the filename is necessary to ensure each file is uniquely saved.        \n",
    "        if window_size < 60:          \n",
    "            win_sec = st_ppg.second\n",
    "\n",
    "        # Extract data for the current window\n",
    "        ppg_win_data = PPG_df.loc[(PPG_df['localtime'] >= st_ppg) & (PPG_df['localtime'] < et_ppg), ['ppg1']]\n",
    "        acc_win_data = ACC_df.loc[(ACC_df['localtime'] >= st_acc) & (ACC_df['localtime'] < et_acc)]\n",
    "\n",
    "        # Check if data meets minimum length requirements\n",
    "        if len(ppg_win_data) < (window_size * 100) or (len(acc_win_data) / (window_size * 50)) < 0.95:\n",
    "            st_ppg = et_ppg\n",
    "            et_ppg = st_ppg + timedelta(seconds=window_size)\n",
    "            st_acc = et_acc\n",
    "            et_acc = st_acc + timedelta(seconds=window_size)\n",
    "            continue\n",
    "\n",
    "        # Downsample PPG data to 50 Hz to match ACC data frequency\n",
    "        ppg_win_data = ppg_win_data.values[:window_size * 100][::2]\n",
    "\n",
    "        if len(acc_win_data) >= window_size * 50:\n",
    "            acc_win_data = acc_win_data[['x', 'y', 'z']].values[:window_size * 50]\n",
    "        else:\n",
    "            # Perform linear interpolation on ACC data\n",
    "            orig_ts = pd.to_datetime(acc_win_data['localtime'].values)\n",
    "            new_ts = pd.date_range(start=st_acc, end=et_acc, freq='20ms', inclusive='left')\n",
    "            closest_before_idx = np.searchsorted(orig_ts, new_ts, side='right') - 1\n",
    "            closest_before_idx = np.clip(closest_before_idx, 0, len(orig_ts) - 2)\n",
    "            closest_after_idx = closest_before_idx + 1\n",
    "\n",
    "            ts1 = orig_ts[closest_before_idx]\n",
    "            ts2 = orig_ts[closest_after_idx]\n",
    "\n",
    "            ts1_delta = (ts1 - pd.Timestamp(0)).total_seconds()\n",
    "            ts2_delta = (ts2 - pd.Timestamp(0)).total_seconds()\n",
    "            new_ts_delta = (new_ts - pd.Timestamp(0)).total_seconds()\n",
    "\n",
    "            # Interpolate each axis ('x', 'y', 'z') separately\n",
    "            resampled_acc_win_data_x = np.array([\n",
    "                np.interp(new_ts_delta[i], [ts1_delta[i], ts2_delta[i]], [acc_win_data['x'].iloc[closest_before_idx[i]], acc_win_data['x'].iloc[closest_after_idx[i]]])\n",
    "                for i in range(len(new_ts_delta))])\n",
    "\n",
    "            resampled_acc_win_data_y = np.array([\n",
    "                np.interp(new_ts_delta[i], [ts1_delta[i], ts2_delta[i]], [acc_win_data['y'].iloc[closest_before_idx[i]], acc_win_data['y'].iloc[closest_after_idx[i]]])\n",
    "                for i in range(len(new_ts_delta))])\n",
    "\n",
    "            resampled_acc_win_data_z = np.array([\n",
    "                np.interp(new_ts_delta[i], [ts1_delta[i], ts2_delta[i]], [acc_win_data['z'].iloc[closest_before_idx[i]], acc_win_data['z'].iloc[closest_after_idx[i]]])\n",
    "                for i in range(len(new_ts_delta))])\n",
    "\n",
    "            acc_win_data = np.vstack([resampled_acc_win_data_x, resampled_acc_win_data_y, resampled_acc_win_data_z]).T\n",
    "\n",
    "        # Combine PPG and ACC data\n",
    "        win_combined = np.hstack((ppg_win_data, acc_win_data))\n",
    "\n",
    "        # Update time window\n",
    "        st_ppg = et_ppg\n",
    "        et_ppg = st_ppg + timedelta(seconds=window_size)\n",
    "        st_acc = et_acc\n",
    "        et_acc = st_acc + timedelta(seconds=window_size)\n",
    "\n",
    "        # Determine file name for saved data\n",
    "        if window_size < 60:\n",
    "            file_name = f\"{user}/{day}/{day_hr}/{day_hr}:{win_min}:{win_sec}.npy\"\n",
    "        else:    \n",
    "            file_name = f\"{user}/{day}/{day_hr}/{day_hr}:{win_min}.npy\"\n",
    "        save_path = os.path.join(output_np_dir, file_name)\n",
    "        \n",
    "        # Ensure the directory exists and save the combined data\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        np.save(save_path, win_combined)\n",
    "\n",
    "# Directories for PPG and ACC data\n",
    "PPG_df_directory = os.path.join(os.getcwd(), 'dfs_ppg_100_days')\n",
    "ACC_df_directory = os.path.join(os.getcwd(), 'dfs_acc_100_days')\n",
    "output_np_dir = os.path.join(os.getcwd(), \"PpgAcc_resampled_10sec_combined_100_days\")\n",
    "os.makedirs(output_np_dir, exist_ok=True)\n",
    "\n",
    "window_size = 10\n",
    "rows = [row for _, row in PPG_ACC_available.iterrows()]\n",
    "\n",
    "# Process in parallel using joblib\n",
    "list(\n",
    "    tqdm(\n",
    "        Parallel(return_as=\"generator\", n_jobs=50)(\n",
    "            delayed(combined_ppg_acc)(row, PPG_df_directory, ACC_df_directory, output_np_dir, window_size) \n",
    "            for row in tqdm(rows, desc=\"Processing ppg and acc hours\")\n",
    "        ),\n",
    "        total=len(rows)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84ffa7",
   "metadata": {},
   "source": [
    "##### Split 120 participants for train, val and test sets with a ratio of 70:15:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce0753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 18 18\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# PpgAcc_resampled_combined_100_days has the data for 4-min windows\n",
    "all_ptcs =  sorted(os.listdir(str(os.path.join(os.getcwd(), 'PpgAcc_resampled_combined_100_days'))))\n",
    "all_ptcs_shf = np.random.permutation(all_ptcs).tolist()\n",
    "\n",
    "n = len(all_ptcs_shf)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "\n",
    "train_end = int(n * train_ratio)\n",
    "val_end = train_end + int(n * val_ratio)\n",
    "\n",
    "train_ptcs = all_ptcs_shf[:train_end]\n",
    "val_ptcs = all_ptcs_shf[train_end:val_end]\n",
    "test_ptcs = all_ptcs_shf[val_end:]\n",
    "\n",
    "print(len(train_ptcs), len(val_ptcs), len(test_ptcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85dcd0",
   "metadata": {},
   "source": [
    "##### Divide participants' directories corresponding to train, test and val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def create_data_directories(base_dir: Union[str, os.PathLike]) -> None:\n",
    "    \"\"\"\n",
    "    Creates a base directory and subdirectories for training, validation, and testing data.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir (str or os.PathLike): The path to the base directory to be created. \n",
    "      Subdirectories 'train', 'val', and 'test' will be created within this directory.\n",
    "      \n",
    "    The function ensures that if the base directory does not exist, it is created, \n",
    "    along with the 'train', 'val', and 'test' subdirectories.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(os.getcwd(), base_dir)\n",
    "\n",
    "    # Create the base directory if it doesn't exist\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # List of subdirectories to create within the base directory\n",
    "    subdirs = ['train', 'val', 'test']\n",
    "\n",
    "    # Create each subdirectory\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(base_path, subdir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_folder_recursively(user: str, src: Union[str, os.PathLike], dst: Union[str, os.PathLike]) -> None:\n",
    "    \"\"\"\n",
    "    Copies a folder and its contents recursively to a destination folder.\n",
    "\n",
    "    Args:\n",
    "    - user (str): The user identifier, used to specify subfolders.\n",
    "    - src (Union[str, os.PathLike]): The source folder to copy.\n",
    "    - dst (Union[str, os.PathLike]): The destination folder.\n",
    "    \"\"\"\n",
    "    src = os.path.join(src, f\"{user}\")\n",
    "    dst = os.path.join(dst, f\"{user}\")\n",
    "    try:\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copytree(src, dst)\n",
    "            print(f\"Successfully copied {src} to {dst}\")\n",
    "        else:\n",
    "            print(f\"Destination folder {dst} already exists.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error copying {src} to {dst}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "\n",
    "def copy_data(src: str, dst: str, participants: List[str], data_type: str, n_jobs: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    Copies folders for participants to a specified destination type (train, val, test) in a distributed manner.\n",
    "    \n",
    "    Parameters:\n",
    "    - src (str): Source directory containing the data.\n",
    "    - dst (str): Destination directory for the current data type.\n",
    "    - participants (List[str]): List of participant identifiers.\n",
    "    - data_type (str): The type of data ('train', 'val', 'test') for copying.\n",
    "    - n_jobs (int): Number of parallel jobs to run. Default is 50.\n",
    "    \"\"\"\n",
    "    list(\n",
    "        tqdm(\n",
    "            Parallel(return_as=\"generator\", n_jobs=n_jobs)(\n",
    "                delayed(copy_folder_recursively)(user, src, dst) \n",
    "                for user in tqdm(participants, desc=f\"Creating {data_type} data\")\n",
    "            ),\n",
    "            total=len(participants)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test sub-directories for input window size of 10 seconds\n",
    "src = os.path.join(os.getcwd(), \"PpgAcc_resampled_10sec_combined_100_days\")\n",
    "base_dst = 'ppg_acc_np_10sec'\n",
    "\n",
    "# Create necessary directories once\n",
    "create_data_directories(base_dst)\n",
    "\n",
    "# Paths for different data types\n",
    "train_dst = os.path.join(os.getcwd(), base_dst, 'train')\n",
    "val_dst = os.path.join(os.getcwd(), base_dst, 'val')\n",
    "test_dst = os.path.join(os.getcwd(), base_dst, 'test')\n",
    "\n",
    "# Copy data for each set\n",
    "copy_data(src, train_dst, train_ptcs, 'train')\n",
    "copy_data(src, val_dst, val_ptcs, 'val')\n",
    "copy_data(src, test_dst, test_ptcs, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f30fe",
   "metadata": {},
   "source": [
    "##### For each of the below cells, first execute combined_ppg_acc function with appropriate window size and destination folder for all users' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ff2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test sub-directories for input window size of 1 minute\n",
    "src = os.path.join(os.getcwd(), \"PpgAcc_resampled_1min_combined_100_days\")\n",
    "base_dst = 'ppg_acc_np_1min'\n",
    "\n",
    "# Create necessary directories once\n",
    "create_data_directories(base_dst)\n",
    "\n",
    "# Paths for different data types\n",
    "train_dst = os.path.join(os.getcwd(), base_dst, 'train')\n",
    "val_dst = os.path.join(os.getcwd(), base_dst, 'val')\n",
    "test_dst = os.path.join(os.getcwd(), base_dst, 'test')\n",
    "\n",
    "# Copy data for each set\n",
    "copy_data(src, train_dst, train_ptcs, 'train')\n",
    "copy_data(src, val_dst, val_ptcs, 'val')\n",
    "copy_data(src, test_dst, test_ptcs, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404989d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test sub-directories for input window size of 2 minutes\n",
    "src = os.path.join(os.getcwd(), \"PpgAcc_resampled_2min_combined_100_days\")\n",
    "base_dst = 'ppg_acc_np_2min'\n",
    "\n",
    "# Create necessary directories once\n",
    "create_data_directories(base_dst)\n",
    "\n",
    "# Paths for different data types\n",
    "train_dst = os.path.join(os.getcwd(), base_dst, 'train')\n",
    "val_dst = os.path.join(os.getcwd(), base_dst, 'val')\n",
    "test_dst = os.path.join(os.getcwd(), base_dst, 'test')\n",
    "\n",
    "# Copy data for each set\n",
    "copy_data(src, train_dst, train_ptcs, 'train')\n",
    "copy_data(src, val_dst, val_ptcs, 'val')\n",
    "copy_data(src, test_dst, test_ptcs, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test sub-directories for input window size of 4 minutes\n",
    "src = os.path.join(os.getcwd(), \"PpgAcc_resampled_combined_100_days\")\n",
    "base_dst = 'ppg_acc_np'\n",
    "\n",
    "# Create necessary directories once\n",
    "create_data_directories(base_dst)\n",
    "\n",
    "# Paths for different data types\n",
    "train_dst = os.path.join(os.getcwd(), base_dst, 'train')\n",
    "val_dst = os.path.join(os.getcwd(), base_dst, 'val')\n",
    "test_dst = os.path.join(os.getcwd(), base_dst, 'test')\n",
    "\n",
    "# Copy data for each set\n",
    "copy_data(src, train_dst, train_ptcs, 'train')\n",
    "copy_data(src, val_dst, val_ptcs, 'val')\n",
    "copy_data(src, test_dst, test_ptcs, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmstress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
