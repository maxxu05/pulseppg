{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32ff7d",
   "metadata": {},
   "source": [
    "##### Processing PPG Data from Parquet Files\n",
    "\n",
    "<ul style=\"font-size: 0.8em;\">\n",
    "This code snippet processes Photoplethysmography (PPG) data stored in Parquet files. It extracts relevant data columns from each file, consolidates the information, and saves the structured data as pickle files. The key steps include:\n",
    "\n",
    "  <li><strong>Reading Parquet Files:</strong> The script reads specific columns from each Parquet file to optimize performance.</li>\n",
    "  <li><strong>Data Consolidation:</strong> Data from multiple files is concatenated into a single DataFrame, ensuring consistency and adding user identifiers.</li>\n",
    "  <li><strong>Data Cleaning:</strong> The DataFrame is cleaned by removing duplicates and sorting by local time.</li>\n",
    "  <li><strong>Grouping and Saving:</strong> The cleaned data is grouped by day and start hour, then each group is saved as a separate pickle file.</li>\n",
    "\n",
    "This process is executed for each user directory in parallel, leveraging multiple cores for efficient processing.  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45927bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def extract_ppg_df(udir: str, input_dir: Union[str, os.PathLike], output_dir: Union[str, os.PathLike]) -> None:\n",
    "    \"\"\"\n",
    "    Extracts and processes PPG data from Parquet files in the specified user directory, \n",
    "    and saves the processed data as pickle files.\n",
    "\n",
    "    Args:\n",
    "    - udir (str): User directory name, containing the PPG data files.\n",
    "    - input_dir (Union[str, os.PathLike]): The input directory path where user directories are located.\n",
    "    - output_dir (Union[str, os.PathLike]): The output directory path where processed data will be saved.\n",
    "\n",
    "    The function reads specific columns from Parquet files, consolidates them into a DataFrame, \n",
    "    adds user information, cleans the data, groups it by day and start time, and saves each group to a pickle file.\n",
    "    \"\"\"\n",
    "    # Construct the full path to the user's directory\n",
    "    udir_path = os.path.join(input_dir, udir)\n",
    "    \n",
    "    # Extract user identifier from directory name\n",
    "    user = udir.split('=')[1]\n",
    "    \n",
    "    all_data = []     \n",
    "    files = [file for file in os.listdir(udir_path) if file.endswith('parquet')]\n",
    "    \n",
    "    # Process each Parquet file in the user's directory\n",
    "    for file in tqdm(files, desc=f\"Processing parquet files for {udir}\", leave=True):\n",
    "        filepath = os.path.join(udir_path, file)\n",
    "        df = pd.read_parquet(filepath, columns=['localtime', 'ppg1', 'day', 'start_lt', 'end_lt', 'label'])\n",
    "        all_data.append(df)\n",
    "            \n",
    "    ppg_df = pd.concat(all_data, ignore_index=True)\n",
    "    ppg_df['user'] = user        \n",
    "    \n",
    "    # Clean the DataFrame by removing duplicates and sorting by localtime\n",
    "    ppg_df = (ppg_df\n",
    "              .drop_duplicates()\n",
    "              .sort_values(by='localtime')\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    grp_dfs = ppg_df.groupby(['day', 'start_lt'])    \n",
    "    for (day, start_lt), df in grp_dfs:\n",
    "        label = df['label'].iloc[0]        \n",
    "        with open(os.path.join(output_dir, f\"{user}_{day}_{start_lt}_{label}.pickle\"), 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "\n",
    "# Set input and output directory paths\n",
    "input_dir = os.path.join(os.getcwd(), 'act_eval_ppg')\n",
    "output_dir = os.path.join(os.getcwd(), 'dfs_motion_ppg')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "user_dirs = [udir for udir in os.listdir(input_dir) if udir.startswith('user')]\n",
    "# Process each user directory in parallel\n",
    "list(\n",
    "    tqdm(\n",
    "        Parallel(return_as=\"generator\", n_jobs=20)(\n",
    "            delayed(extract_ppg_df)(udir, input_dir, output_dir) \n",
    "            for udir in tqdm(user_dirs, desc=\"Processing for uid: {udir}\")\n",
    "        ),\n",
    "        total=len(user_dirs),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adb27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe of file names of stored pickle files.\n",
    "\n",
    "# output_dir = os.path.join(os.getcwd(), 'dfs_motion_ppg')\n",
    "output_dir = \"/data/mithun/dfs_motion_ppg\"\n",
    "PPG_available = []\n",
    "for filename in os.listdir(output_dir):\n",
    "    PPG_available.append(filename.split('.')[0].split('_'))\n",
    "\n",
    "PPG_available_df = (pd.DataFrame(PPG_available, columns=['user', 'day', 'st_time', 'label'])\n",
    "                    .astype({'user': str, \n",
    "                             'day': 'datetime64[ns]',  # covert to datetime from object type\n",
    "                             'st_time': 'datetime64[ns]', # covert to datetime from object type\n",
    "                             'label': 'str'})\n",
    "                    .sort_values(by=['user', 'day', 'st_time']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07a736",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c50214",
   "metadata": {},
   "source": [
    "##### Split 120 participants for train, val and test sets with a ratio of 70:15:15; this same split of pariticipants is used during training also. The test set was kept hidden during the self-spervised training phase to prevent data leakage during downstream task of activity detection in the field. However, since we used logistic regression from scikit-learn for the activity detection downstream task, during model building train and val splits were merged while the test split was used for model evaluation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3dcdefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 18 18\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "udirs =  sorted(os.listdir(str(os.path.join(os.getcwd(), 'act_eval_ppg'))))\n",
    "all_ptcs = [udir.split(\"=\")[1] for udir in udirs if udir.startswith('user')]\n",
    "all_ptcs_shf = np.random.permutation(all_ptcs).tolist()\n",
    "\n",
    "n = len(all_ptcs_shf)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "\n",
    "train_end = int(n * train_ratio)\n",
    "val_end = train_end + int(n * val_ratio)\n",
    "\n",
    "train_ptcs = all_ptcs_shf[:train_end]\n",
    "val_ptcs = all_ptcs_shf[train_end:val_end]\n",
    "test_ptcs = all_ptcs_shf[val_end:]\n",
    "\n",
    "print(len(train_ptcs), len(val_ptcs), len(test_ptcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78035f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of users' PPG means, standard deviations, and clipping thresholds    \n",
    "with open (\"dict_user_ppg_mean_std_per.pickle\", \"rb\") as f:\n",
    "    dict_user_ppg_mean_std_per = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be500d",
   "metadata": {},
   "source": [
    "##### Exporting PPG Data to Numpy Files\n",
    "\n",
    "<ul style=\"font-size: 0.8em;\">\n",
    "This code snippet processes and exports Photoplethysmography (PPG) data stored in pickle files to numpy format. It ensures that the data is correctly normalized and clipped according to user-specific statistics. Key steps include:\n",
    "\n",
    "  <li><strong>File Reading and Sorting:</strong> The PPG data is read from pickle files and sorted by local time for consistency.</li>\n",
    "  <li><strong>Interpolation:</strong> When data length is insufficient, interpolation fills the gaps using a 20ms frequency.</li>\n",
    "  <li><strong>Clipping and Normalization:</strong> Data is clipped to user-specific thresholds and normalized using mean and standard deviation.</li>\n",
    "  <li><strong>Saving to Numpy:</strong> The processed data is saved as a numpy file with a structured naming convention for easy identification.</li>\n",
    "\n",
    "The process is applied to each file, optimizing data handling and storage for subsequent analysis and modeling.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Union\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def export_numpy_file(\n",
    "    file_path: str,\n",
    "    dfs_dirs: Union[str, os.PathLike],\n",
    "    output_np_dir: Union[str, os.PathLike],\n",
    "    dict_user_ppg_mean_std_per: Dict[str, Tuple[float, float, float]],\n",
    "    clip: bool,\n",
    "    norm: bool\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Exports PPG data to a numpy file after processing and normalizing/clipping based on user-specific statistics.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the pickle file containing the PPG DataFrame.\n",
    "    - dfs_dirs (Union[str, os.PathLike]): Directory containing the pickle files.\n",
    "    - output_np_dir (Union[str, os.PathLike]): Directory to save the output numpy files.\n",
    "    - dict_user_ppg_mean_std_per (Dict[str, Tuple[float, float, float]]): Dictionary of user-specific PPG mean, \n",
    "      standard deviation, and clipping threshold.\n",
    "    - clip (bool): Whether to apply clipping to PPG data.\n",
    "    - norm (bool): Whether to normalize PPG data using user-specific statistics.\n",
    "\n",
    "    This function reads a pickle file, processes the PPG data to interpolate if necessary, applies clipping and normalization\n",
    "    based on user-specific statistics, and saves the result as a numpy file.\n",
    "    \"\"\"\n",
    "    # Load PPG data from a pickle file\n",
    "    with open(os.path.join(dfs_dirs, file_path), 'rb') as f:\n",
    "        ppg_df = pickle.load(f)\n",
    "\n",
    "    # Sort the DataFrame by localtime to ensure chronological order\n",
    "    ppg_df = ppg_df.sort_values(by=['localtime'])\n",
    "    user = ppg_df['user'].iloc[0]        \n",
    "    day = ppg_df['day'].iloc[0]\n",
    "    st_win = ppg_df['start_lt'].iloc[0]\n",
    "\n",
    "    # Retrieve user-specific statistics for normalization and clipping\n",
    "    user_ppg_mean, user_ppg_std, clip_tr = dict_user_ppg_mean_std_per[user]\n",
    "    \n",
    "    # Check if the data length is sufficient (at least 95% of 2000 entries)\n",
    "    if len(ppg_df) / 2000 >= 0.95: # 100 Hz; 20 seconds = 20 * 100 = 2000 datapoints\n",
    "        if len(ppg_df) >= 2000: \n",
    "            # Downsample the PPG data if sufficient\n",
    "            ppg_win_data = ppg_df['ppg1'].values[:2000][::2]\n",
    "        else:\n",
    "            # Perform interpolation if the data is insufficient\n",
    "            st = ppg_df['start_lt'].iloc[0]\n",
    "            et = ppg_df['end_lt'].iloc[-1]\n",
    "            orig_ts = ppg_df['localtime'].values\n",
    "            new_ts = pd.date_range(start=st, end=et, freq='20ms', inclusive='left')\n",
    "            orig_ts = pd.to_datetime(orig_ts)\n",
    "\n",
    "            # Find closest indices before and after each new timestamp for interpolation\n",
    "            closest_before_idx = np.searchsorted(orig_ts, new_ts, side='right') - 1\n",
    "            closest_before_idx = np.clip(closest_before_idx, 0, len(orig_ts) - 2)\n",
    "            closest_after_idx = closest_before_idx + 1\n",
    "\n",
    "            # Extract timestamps and values for interpolation\n",
    "            ts1 = orig_ts[closest_before_idx]\n",
    "            ts2 = orig_ts[closest_after_idx]\n",
    "\n",
    "            # Convert timestamps to seconds for interpolation\n",
    "            ts1_delta = (ts1 - pd.Timestamp(0)).total_seconds()\n",
    "            ts2_delta = (ts2 - pd.Timestamp(0)).total_seconds()\n",
    "            new_ts_delta = (new_ts - pd.Timestamp(0)).total_seconds()\n",
    "\n",
    "            # Interpolate PPG data for the new timestamps\n",
    "            ppg_win_data = np.array([\n",
    "                np.interp(new_ts_delta[i], [ts1_delta[i], ts2_delta[i]], \n",
    "                          [ppg_df['ppg1'].iloc[closest_before_idx[i]], ppg_df['ppg1'].iloc[closest_after_idx[i]]])\n",
    "                for i in range(len(new_ts_delta))\n",
    "            ])\n",
    "\n",
    "        if clip:\n",
    "            ppg_win_data = np.where(ppg_win_data > clip_tr, clip_tr, ppg_win_data)\n",
    "        if norm:\n",
    "            ppg_win_data -= user_ppg_mean\n",
    "            ppg_win_data /= user_ppg_std\n",
    "        \n",
    "        # Construct the file name and save path\n",
    "        file_name = f\"{user}_{day}_{st_win}.npy\"\n",
    "        save_path = os.path.join(output_np_dir, file_name)        \n",
    "        np.save(save_path, ppg_win_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb0ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "\n",
    "def create_data_directories(base_dir: Union[str, os.PathLike]) -> None:\n",
    "    \"\"\"\n",
    "    Creates a base directory and subdirectories for training, validation, and testing data,\n",
    "    including 'noise' and 'noisefree' categories within each subdirectory.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir (Union[str, os.PathLike]): The path to the base directory to be created. \n",
    "      Subdirectories 'train', 'val', and 'test' with 'noise' and 'noisefree' categories\n",
    "      will be created within this directory.\n",
    "      \n",
    "    The function ensures that all necessary directories are created if they do not already exist.\n",
    "    \"\"\"\n",
    "    # Create the base directory if it doesn't exist\n",
    "    base_path = os.path.join(os.getcwd(), base_dir)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Define the structure of subdirectories needed\n",
    "    subdirs = ['train', 'val', 'test']\n",
    "    categories = ['noise', 'noisefree']\n",
    "\n",
    "    # Create each subdirectory and its categories\n",
    "    for subdir in subdirs:\n",
    "        for category in categories:\n",
    "            os.makedirs(os.path.join(base_path, subdir, category), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labelwise directories for train, val and test sets\n",
    "create_data_directories('motion_ppg_np')\n",
    "dfs_dirs = os.path.join(os.getcwd(), 'dfs_motion_ppg') # location of pickle files with ppg data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19912c",
   "metadata": {},
   "source": [
    "##### Processing PPG Data for Machine Learning\n",
    "\n",
    "<ul style=\"font-size: 0.8em;\">\n",
    "This function processes Photoplethysmography (PPG) data for different participant groups and labels, exporting them as numpy files and loading them for use in machine learning tasks. It handles various steps including filtering, file path generation, parallel processing, and data labeling. Key steps include:\n",
    "\n",
    "  <li><strong>Data Filtering:</strong> Selects data for specific participants and labels, ensuring relevant subsets are processed.</li>\n",
    "  <li><strong>File Path Construction:</strong> Dynamically generates file paths based on metadata, facilitating organized data handling.</li>\n",
    "  <li><strong>Parallel Processing:</strong> Utilizes parallel computing to efficiently process multiple files simultaneously.</li>\n",
    "  <li><strong>Data Export and Load:</strong> Saves processed data as numpy files and loads them into arrays with corresponding labels for classification tasks.</li>\n",
    "\n",
    "This approach standardizes the processing pipeline, making it adaptable for different datasets and labels, ensuring efficient data handling for machine learning applications.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_ppg_data(\n",
    "    PPG_available_df: pd.DataFrame,\n",
    "    participants: List[str],\n",
    "    label: str,\n",
    "    output_subdir: str,\n",
    "    dfs_dirs: Union[str, os.PathLike],\n",
    "    dict_user_ppg_mean_std_per: Dict[str, Tuple[float, float, float]],\n",
    "    clip: bool = True,\n",
    "    norm: bool = True,\n",
    "    n_jobs: int = 30\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Processes PPG data for participants, exports to numpy files, and loads them into numpy arrays for classification.\n",
    "\n",
    "    Args:\n",
    "    - PPG_available_df (pd.DataFrame): DataFrame containing metadata about available PPG data.\n",
    "    - participants (List[str]): List of participant identifiers.\n",
    "    - label (str): Label for the type of data ('noise' or 'noisefree').\n",
    "    - output_subdir (str): Subdirectory under 'motion_ppg_np' for saving processed data ('test', 'val', 'train').\n",
    "    - dfs_dirs (Union[str, os.PathLike]): Directory containing the pickled DataFrame files.\n",
    "    - dict_user_ppg_mean_std_per (Dict[str, Tuple[float, float, float]]): Dictionary with user-specific statistics.\n",
    "    - clip (bool): Whether to apply clipping to PPG data.\n",
    "    - norm (bool): Whether to normalize PPG data using user-specific statistics.\n",
    "    - n_jobs (int): Number of parallel jobs for processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: The processed PPG data and their corresponding labels.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the specified participants and label\n",
    "    filtered_df = PPG_available_df.loc[(PPG_available_df['user'].isin(participants)) & \n",
    "                                       (PPG_available_df['label'] == label)]\n",
    "    \n",
    "    # Generate file paths for the pickle files\n",
    "    file_paths = [\n",
    "        os.path.join(f\"{row['user']}_{row['day'].date()}_{row['st_time']}_{row['label']}.pickle\")\n",
    "        for idx, row in filtered_df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Define output directory for processed numpy files\n",
    "    output_np_dir = os.path.join(os.getcwd(), 'motion_ppg_np', output_subdir, label)\n",
    "    \n",
    "    # Process each file and save as a numpy file\n",
    "    list(\n",
    "        tqdm(\n",
    "            Parallel(return_as=\"generator\", n_jobs=n_jobs)(\n",
    "                delayed(export_numpy_file)(file_path, dfs_dirs, output_np_dir, dict_user_ppg_mean_std_per, clip, norm) \n",
    "                for file_path in tqdm(file_paths, desc=\"Processing pickle files\")\n",
    "            ),\n",
    "            total=len(file_paths)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Load the processed numpy files into an array\n",
    "    ppg_X = []\n",
    "    for file in os.listdir(output_np_dir):\n",
    "        ppg_X.append(np.load(os.path.join(output_np_dir, file)))\n",
    "    \n",
    "    # Generate labels for the data\n",
    "    ppg_y = [1 if label == 'noise' else 0] * len(ppg_X)  # 1 for noise, 0 for noisefree\n",
    "    \n",
    "    return ppg_X, ppg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dcf79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process train data for activity detection in the field\n",
    "train_X_noise, train_y_noise = process_ppg_data(\n",
    "    PPG_available_df, train_ptcs, 'noise', 'train', dfs_dirs, dict_user_ppg_mean_std_per\n",
    ")\n",
    "\n",
    "train_X_noisefree, train_y_noisefree = process_ppg_data(\n",
    "    PPG_available_df, train_ptcs, 'noisefree', 'train', dfs_dirs, dict_user_ppg_mean_std_per\n",
    ")\n",
    "\n",
    "# Combine the results\n",
    "train_X = np.array(train_X_noisefree + train_X_noise)\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, -1)\n",
    "train_y = np.array(train_y_noisefree + train_y_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81009cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2597670, 1, 1000), (2597670,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058c2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process val data for activity detection in the field\n",
    "val_X_noise, val_y_noise = process_ppg_data(\n",
    "    PPG_available_df, val_ptcs, 'noise', 'val', dfs_dirs, dict_user_ppg_mean_std_per\n",
    ")\n",
    "\n",
    "val_X_noisefree, val_y_noisefree = process_ppg_data(\n",
    "    PPG_available_df, val_ptcs, 'noisefree', 'val', dfs_dirs, dict_user_ppg_mean_std_per\n",
    ")\n",
    "\n",
    "# Combine the results\n",
    "val_X = np.array(val_X_noisefree + val_X_noise)\n",
    "val_X = val_X.reshape(val_X.shape[0], 1, -1)\n",
    "val_y = np.array(val_y_noisefree + val_y_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c204ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((499345, 1, 1000), (499345,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2f08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data for activity detection in the field\n",
    "test_X_noise, test_y_noise = process_ppg_data(\n",
    "    PPG_available_df, test_ptcs, 'noise', 'test', dfs_dirs, dict_user_ppg_mean_std_per\n",
    ")\n",
    "\n",
    "test_X_noisefree, test_y_noisefree = process_ppg_data(\n",
    "    PPG_available_df, test_ptcs, 'noisefree', 'test', dfs_dirs, dict_user_ppg_mean_std_per\n",
    ")\n",
    "\n",
    "# Combine the results\n",
    "test_X = np.array(test_X_noisefree + test_X_noise)\n",
    "test_X = test_X.reshape(test_X.shape[0], 1, -1)\n",
    "test_y = np.array(test_y_noisefree + test_y_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0041cb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((462414, 1, 1000), (462414,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45167629",
   "metadata": {},
   "source": [
    "##### Save train, val and test numpy files to be used for model building for the downstream task of activity detection in the field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e038a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_dir = os.path.join(os.getcwd(), \"downstream\", \"stnst\", \"ppg_clp_norm\")\n",
    "if not os.path.exists(model_data_dir):\n",
    "    os.makedirs(model_data_dir)\n",
    "\n",
    "file_name = \"train_X_ppg.npy\"\n",
    "np.save(os.path.join(model_data_dir, file_name), train_X)\n",
    "file_name = \"val_X_ppg.npy\"\n",
    "np.save(os.path.join(model_data_dir, file_name), val_X)\n",
    "file_name = \"test_X_ppg.npy\"\n",
    "np.save(os.path.join(model_data_dir, file_name), test_X)\n",
    "\n",
    "file_name = \"train_y_stationary_nonstationary.npy\"\n",
    "np.save(os.path.join(model_data_dir, file_name), train_y)\n",
    "file_name = \"val_y_stationary_nonstationary.npy\"\n",
    "np.save(os.path.join(model_data_dir, file_name), val_y)\n",
    "file_name = \"test_y_stationary_nonstationary.npy\"\n",
    "np.save(os.path.join(model_data_dir, file_name), test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmstress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
